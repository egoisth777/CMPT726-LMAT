\section{Neural Network and Non-Linear Optimizations}
\begin{enumerate}
\item \textbf{\em Neural Networks}

We will perform the forward and backward passes on the neural network below.

{\bf Notation:}
Please use the notation following the examples of names for weights given in the figure.  For instance, the red node would have
pre-activation of $z_2^{(1)} = w_{2,1}^{(0)}x_1 + w_{2,2}^{(0)}x_2 + w_{2,3}^{(0)}x_3$ and post-activation of $h_2^{(1)} = g(z_2^{(1)})$. 
In this question, the superscript will refer to the network layer the variable belongs to. In vector notation, we would have $\vv{z}^{(1)} = W^{(0)} \vv x$, where:

\begin{equation}
    \vv{z}^{(1)} = \begin{pmatrix} z_1^{(1)} \\ z_2^{(1)} \\ z_3^{(1)}\\ \end{pmatrix}, \quad \vv{x} = \begin{pmatrix} x_1 \\ x_2 \\ x_3\\ \end{pmatrix}, \quad W^{(0)} = \begin{bmatrix}w^{(0)}_{1,1} & w^{(0)}_{1,2} & w^{(0)}_{1,3} \\ w^{(0)}_{2,1} & w^{(0)}_{2,2} & w^{(0)}_{2,3} \\ w^{(0)}_{3,1} & w^{(0)}_{3,2} & w^{(0)}_{3,3} \end{bmatrix}
\end{equation}

{\bf Activation functions:}
Assume the activation functions $g(\cdot)$ for the hidden layers are Sigmoid.  For the final output node assume the activation function
is an identity function $g(a)=a$.

{\bf Loss function:}
Assume this network is doing regression, trained using the standard
squared error for a given data point $(\vv{x},y)$ so that $L(\vv{W}) = \frac{1}{2} (\hat{y}-y)^2$.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figs/neural_networks.pdf}
\label{fig:f1}
\end{figure}

\begin{enumerate}
\item Compute the outputs of the middle layers and the final output of the network $\hat{y}$ for the given input vector $\vv{x} = (1,1,1)^\top$ and the following weights. There are no biases. (\textbf{3 marks})

\begin{equation*}
    W^{(0)} = \begin{bmatrix}0.1 & 0.1 & 0.1 \\ 0.1 & 0.1 & 0.1 \\ 0.1 & 0.1 & 0.1 \end{bmatrix}, \quad W^{(1)} = \begin{bmatrix}0.3 & 0.3 & 0.3 \\ 0.3 & 0.3 & 0.3 \\ 0.3 & 0.3 & 0.3 \end{bmatrix}, \quad W^{(2)} = [0.5,0.5,0.5]
\end{equation*}

\item  Compute $$
\frac{\partial L}{\partial  \hat y}, \frac{\partial L}{\partial \vv{z}^{(2)}},
\frac{\partial L}{\partial \vv{z}^{(1)}}
$$ (\textbf{6 marks})

\item Compute $$
\frac{\partial L}{\partial  W^{(0)}}, \frac{\partial L}{\partial W^{(1)}},
\frac{\partial L}{\partial W^{(2)}}
$$ Use this result to update all the weights in the network using gradient descent after one step if the ground truth label is $y=2$, given the above-mentioned initial weights, and learning rate of 1 (\textbf{6 marks}).
\end{enumerate}

\item \textbf{\em Non-linear Optimization}

In this question, you need to implement iterative algorithms to solve the following nonlinear optimization problem. Finish the codes in this \href{https://colab.research.google.com/drive/1r3jPEEtxCj7bz1CJdGNxBODdWnltDq4G?usp=sharing}{Jupyter Notebook} (there are total 7 "\texttt{\#<<TODO\#x>>}" in this question). Please make a copy of the notebook in Google Drive and run your codes in there.

Consider the following objective function.

\begin{equation}
    f(x,y) = x^2 + \dfrac{(y-2)^2}{2}
    \label{eq: obj_val}
\end{equation}

For the following questions, you can describe the results from the perspective of whether function reaches the minimum value, and the convergence speed, etc.

\begin{enumerate}
\item Implement a basic Gradient Descent algorithm for the objective function \eqref{eq: obj_val}. (\textbf{1 points})
    \begin{itemize}
        \item Use step size of 0.2, total iterations of 100, and initial point of $(-10, 10)$ for the objective function. 
        \item Change the step size to 0.01. Provide plots for both step size and report your observation. 
    \end{itemize}
    
\item Implement the Adaptive Gradient (AdaGrad) algorithm for the objective function \eqref{eq: obj_val}. (\textbf{3 points})
    \begin{itemize}
        \item Use step size of 0.2, total iterations of 100, and initial point of $(-10, 10)$ for the objective function.
        \item  Change the step size to 0.3. Provide plots for both step size and report your observation.
    \end{itemize}
    
\item Implement the Adam algorithm for the objective function \eqref{eq: obj_val}. (\textbf{3 points})
    \begin{itemize}
        \item Use step size of 0.2, total iterations of 150, and initial point of $(-10, 10)$ for the objective function. Use default values for other parameters. Does the solution converge to the minimal objective value? Provide the plot and also report your observation.
    \end{itemize}

\item Implement the Newton's Method algorithm for the objective function \eqref{eq: obj_val}. (\textbf{3 points})
    \begin{itemize}
        \item Use step size of 0.01, total iterations of 100, and initial point of $(-10, 10)$ for the objective function.
        \item  Change the step size to 0.1. Provide plots for both step size and report your observation. 
    \end{itemize}

\end{enumerate}
\end{enumerate}
